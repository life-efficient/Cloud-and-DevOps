{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prometheus\n",
    "\n",
    "> __Prometheus is an open source monitoring and alerting toolkit gathering and processing data locally if was orginally developed at SoundCloud and now is a fully open source community driven project. Prometheus is designed to collect and store metrics as time series data, it stores metric information with a timestamp when it was recorded alongside key-value pairs called labels.__\n",
    "\n",
    "To get a broader overview of Prometheus and it's uses you can visit the main page [here](https://prometheus.io/docs/introduction/overview/).\n",
    "\n",
    "A few facts about it:\n",
    "- Written in `golang` a programming lanaguage developed at google!\n",
    "- Provides APIs for different languages (including Python), __but__...\n",
    "- We use our browser to query data using Prometheus specific language called PromQL\n",
    "- The __Prometheus dashboard__ resides on `localhost:9090` by default\n",
    "  \n",
    "\n",
    "## The main components\n",
    "\n",
    "* **Prometheus server:** The server will scrape and store the time series data.\n",
    "* **Client libraries:** for instrumenting application code.\n",
    "* **Push gateway:** for supporting short lived jobs.\n",
    "* **Alertmanager:** To handle alerts.\n",
    "* **Exporters:** for suported services like HAProxy, StatsD, Graphite and many more.\n",
    "\n",
    "![](images/prometheus_architecture.png)\n",
    "\n",
    "## What's going on above?\n",
    "\n",
    "- Prometheus scrapes data (like metrics):\n",
    "    - from short lived jobs via `push gateway`\n",
    "    - from long running jobs directly\n",
    "- __All samples (values with timestamps) are stored locally__ (together with necessary metadata)\n",
    "- Runs predefined rules on collected data:\n",
    "    - Gather and aggregate new records\n",
    "    - Process them and send alerts\n",
    "- Consumers use Prometheus's API to visualise data.\n",
    "\n",
    "## When to use it?\n",
    "\n",
    "- __Recording numerical time series__, this could be:\n",
    "    - various training metrics gathered across epochs/batches\n",
    "    - hardware related data\n",
    "    - network traffic and other statistics\n",
    "- __To debug infrastructure during network outages etc.__ (if interested, you can read about Slack's outage [here](https://slack.engineering/slacks-outage-on-january-4th-2021/))\n",
    "\n",
    "> __Data is stored locally and does not rely on network storage__\n",
    "\n",
    "Due to above, if something fails, you always have access to the data (as each Prometheus server is self-contained and independent).\n",
    "\n",
    "__When shouldn't we use it?__\n",
    "\n",
    "- __You need very detailed data coming really fast__ (per request metrics with a lot of requests from users, exact billing when every milisecond counts etc.)\n",
    "\n",
    "That's because:\n",
    "- Prometheus is desgined to scrape data every few seconds\n",
    "- Data is kept in local storage which can fill up really fast (in this case think about large remote storage and data rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "> __All prometheus data is stored as timestamped timeseries differentiated by metric and (optionally) label__\n",
    "\n",
    "- Metric names and labels should be alphanumerical\n",
    "- __Samples are `float64` (`double`) numerical types__\n",
    "- __Timestamps are in milliseconds__\n",
    "\n",
    "For instance `process_cpu_seconds_total` which will calculate the total cpu usage of our Prometheus instance will be displayed as.\n",
    "\n",
    "`process_cpu_seconds_total{instance=\"localhost:9090\", job=\"prometheus\"}`\n",
    "\n",
    "The general layout of the query is as follows.\n",
    "- ``process_cpu_seconds_total` - metric name\n",
    "- `instance=\"localhost:9090\"` label letting us know the instance which we are checking the cpu time for.\n",
    "- `job=prometheus` another label letting us know the job we are checking the cpu time for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "> Prometheus provides `4` metrics out of the box\n",
    "\n",
    "__Note:__ Prometheus server does not differentiate between metrics (it only keeps the data), metrics are used by the client libraries (once again `4` client libraries provided for `golang`, `java`, __`python`__, `ruby`)\n",
    "\n",
    "### Counter\n",
    "\n",
    "> __Monotonically increasing counter (can be restarted)__\n",
    "\n",
    "Useful for:\n",
    "- number of requests\n",
    "- tasks completed\n",
    "- __anything which can only grow OR start a new__\n",
    "\n",
    "### Gauge\n",
    "\n",
    "> __Single value which can increase & decrease__\n",
    "\n",
    "Useful for:\n",
    "- memory usage monitoring\n",
    "- temperature\n",
    "- __anything which can change value arbitrarily__\n",
    "\n",
    "### Histogram\n",
    "\n",
    "> __Samples observations and groups them in buckets (which you can configure)__\n",
    "\n",
    "Let's say our historgram metric is named `our_super_histogram`. In this case the following operations on histogram are available (__notice we add suffixes to metric name!__):\n",
    "- `our_super_histogram_bucket{le=\"<upper inclusive bound>\"}` - cumulative counters for the observation buckets\n",
    "- `our_super_histogram_sum` - sum of all values\n",
    "- `out_super_histogram_count` - count of observations\n",
    "\n",
    "### Summary\n",
    "\n",
    "> __Samples observations and provides them as a sliding time window__\n",
    "\n",
    "- Provides `sum` and `count` like histogram\n",
    "- `out_super_summary_quantiles{quantile=\"value\"}` - quantile of observations (one can do something similar for histogram but using functions)\n",
    "\n",
    "### Functions\n",
    "\n",
    "There are also functions one can run to query for things like `day_of_month()`, we will talk about them in more detail later\n",
    "\n",
    "## Jobs and instances\n",
    "\n",
    "- __An instance is an endpoint you can scrape data from__, for instance an EC2 instance or a Docker container and will be a single process.\n",
    "- __A job is a collections of the same instances__, like multiple EC2 instances and Docker containers, those are usually replicated for reliability/flexbility.\n",
    "\n",
    "Read more about jobs and instance from the Prometheus documentation [here](https://prometheus.io/docs/concepts/jobs_instances/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First contact\n",
    "\n",
    "## Installation\n",
    "\n",
    "> Prometheus was written in `golang` a programming language developed by Google in 2009, hence __it is contained in a single compiled executable__\n",
    "\n",
    "Due to above, it's installation & deployment is really simple and can be performed efficiently in many different scenarios:\n",
    "\n",
    "- Either go to [their download page](https://prometheus.io/download/) and download the specific version of Prometheus for your operating system.\n",
    "    - if you're on Mac, it's the download labelled Darwin\n",
    "- Or __run prometheus inside Docker container__\n",
    "\n",
    "We will use the last option, since we are running Prometheus from a docker container we will need bind our Prometheus config file to the container. This will allow Prometheus to update the config file using Docker commands. First pull the Prometheus Docker image from Docker Hub using `docker pull prom/prometheus`. Then create a `prometheus.yml` file using the following code, this is just a simple config for you to get started with Prometheus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "global:\n",
    "  scrape_interval: 15s # By default, scrape targets every 15 seconds.\n",
    "  # Attach these labels to any time series or alerts when communicating with\n",
    "  # external systems (federation, remote storage, Alertmanager).\n",
    "  external_labels:\n",
    "    monitor: 'codelab-monitor'\n",
    "\n",
    "# A scrape configuration containing exactly one endpoint to scrape:\n",
    "# Here it's Prometheus itself.\n",
    "scrape_configs:\n",
    "  # The job name added as a label `job=<job_name>` to any timeseries scraped\n",
    "  - job_name: 'prometheus'\n",
    "    # Override the global default and scrape targets from job every 5 seconds.\n",
    "    scrape_interval: '5s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5a23e",
   "metadata": {},
   "source": [
    "\n",
    "We can then build the image using the following command, you will just need to change the `/path/to/prometheus.yml` path to the directory where your `prometheus.yml` config file is stored locally. We also need to add the `--web.enable-lifecycle` flag as it allows the enabling of reloading the config from the command line. Run command below in your command line(if using Windows run the command in Powershell and not bash):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --rm -p 9090:9090 --name prometheus -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus --config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it does:\n",
    "- bind container's `9090` port to `9090` port on the localhost\n",
    "- remove container (`--rm` flag) when you kill the process\n",
    "- `--name` specifies the name of the container\n",
    "- `-v` flag allows us to mount the prometheus config in the container to your local config so we can edit the config on the fly.\n",
    "\n",
    "Prometheus will now be running so we can __simply go to [`localhost:9090`](http://localhost:9090) in your browser__ and you should see the following Prometheus dashboard. You might want to select local time so the metric are logged in your time zone and query history so you can track all queries made. And of course configure dark mode(top right corner) as it's clearly the best!\n",
    "\n",
    "<img src=\"images/prom_init_dash.png?modified=232132453\">\n",
    "\n",
    "If you start typing in the expression field then prometheus will suggest queries you can run. Some metrics will not be configured to be viewable currently but prometheus can track itself. So we could start by running the expressions starting with prometheus. For instance run ``prometheus_build_info`` and and then execute the query. Notice you get details about what the query does when you mouse over it from the dropdown list.\n",
    "\n",
    "<img src=\"images/prom_expression_selected_dashboard.png?modified=232132453\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get the results of the query in the panel underneath in this case the result was. \n",
    "\n",
    "`prometheus_build_info{branch=\"HEAD\", goversion=\"go1.17.1\", instance=\"localhost:9090\", job=\"prometheus\", revision=\"b30db03f35651888e34ac101a06e25d27d15b476\", version=\"2.30.2\"}`\n",
    "\n",
    "- **metric name:** `promethus_build_info` \n",
    "- **labels:** goversion, instance, job etc.\n",
    "\n",
    "You can see a test list of all metrics the Prometheus server is currently logging by going to `http://localhost:9090/metrics.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Try to run `20` Prometheus expressions and figure out what their use is. \n",
    "- Check their graph representation(some expressions may not have graphs for instance `prometheus_build_info`)\n",
    "\n",
    "Check the hints given from the dropdown list to check what they do otherwise use google to find information on the metric if you are unsure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prometheus configuration\n",
    "\n",
    "## Configuration file\n",
    "\n",
    "Until now, we didn't know any details of what just happened, but that's about to change.\n",
    "\n",
    "> __Prometheus server is configured via [YAML](https://en.wikipedia.org/wiki/YAML) files__ the configuration file will be used to define what metrics and instances we would like to scrape. \n",
    "\n",
    "Previously, we ran Prometheus __with default configuration file__ (one can see it in [`localhost:9090/config`](http://localhost:9090/config)) or by selecting **Status > Configuration** from the dashboard .\n",
    "\n",
    "Let's take a look at the config file and get a better idea of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section with default values\n",
    "global:\n",
    "  scrape_interval: 15s # How frequently to scrape targets from jobs\n",
    "  scrape_timeout: 10s # If there is no response from instance do not try to scrape\n",
    "  evaluation_interval: 15s # How frequently to evaluate rules (e.g. reload graphs with new data)\n",
    "# Prometheus alert manager, left for now\n",
    "alerting:\n",
    "  alertmanagers:\n",
    "  - follow_redirects: true\n",
    "    scheme: http\n",
    "    timeout: 10s\n",
    "    api_version: v2\n",
    "    static_configs:\n",
    "    - targets: []\n",
    "# Specific configuration for jobs\n",
    "scrape_configs:\n",
    "- job_name: prometheus # Name of the job, can be anything\n",
    "  honor_timestamps: true # Use timestamps provided by job\n",
    "  scrape_interval: 15s # As before, but for this job\n",
    "  scrape_timeout: 10s # ^\n",
    "  metrics_path: /metrics # Where metrics are located w.r.t. port (localhost:9090/metrics)\n",
    "  scheme: http # Configures the protocol scheme used for requests (localhost is http)\n",
    "  follow_redirects: true\n",
    "  static_configs:\n",
    "  - targets:\n",
    "    - localhost:9090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Prometheus provides A LOT of configuration options, check all of them [here](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config)__\n",
    "\n",
    "A few things to note about config file to keep in mind:\n",
    "- __What's amazing about Prometheus is it reloads it's configuration based on the config file automatically so you can change it live!.__ (you can change it live or even force it to reload by using the `/-/reload` flag.)\n",
    "- __If the file is not correctly formatted IT WILL NOT BE UPDATED__, needs to valid **YAML**.\n",
    "- All parameters that are specified under `global` are avaliable to all other scraper configs defined in the configuration file i.e if you create `scrape_config` that gets metrics from EC2 instances then it will use the global parameters unless explicity changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the `--web.enable-lifecycle` flag when creating our Docker container and mounting our local `prometheus.yml` file to the container we should be able to edit the config live. Let's change the default `scrape_interval` and `scrape_timeout` to 1s. The start of your config should now be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section with default values\n",
    "global:\n",
    "  scrape_interval: 1s # How frequently to scrape targets from jobs\n",
    "  scrape_timeout: 1s # If there is no response from instance do not try to scrape\n",
    "  evaluation_interval: 15s # How frequently to evaluate rules (e.g. reload graphs with new data)\n",
    "# Prometheus alert manager, left for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the `/-/reload` command to update our config while our Prometheus server is running, once you have made your changes run the following command in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:9090/-/reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to `localhost:9090/config` and check you config, it should be updated and you should see the changes that we have made. Excellent we can not edit our config while our Prometheus server is running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [scrape_configs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config)\n",
    "\n",
    "> Specifies __sets of targets__ (what should be scraped) and parameters describing how to do it for each target. For instance you could specify a `scrape_config` to scrape metrics from an EC2 instances, or a config to scrape metrics from Kubernetes.\n",
    "\n",
    "Targets can be defined __statically__ or __dynamically__\n",
    "\n",
    "- `statically` - configured in our configuration YAML in a `scrape_config`.\n",
    "- `dynamically` Using __service discovery configurations__. By defining the service discovery options we can allow Prometheus to track new instances of a particular service. For example in industry you might be starting, stopping, and creating new Docker containers constantly based on the needs of the business. By defining the service discovery configuration Prometheus will be able to track any newly created Docker containers without having to specify them directly in the configuration file.\n",
    "\n",
    "A lot of service discovery integrations are avaliable for commonly used services:\n",
    "- [`consul_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config) - retrieve scraping targets from HashiCorp's [Consul](https://www.consul.io/) used for service discovery and network setup\n",
    "- [`dockerswarm_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config) - used with [Docker's Swarm mode](https://docs.docker.com/engine/swarm/) which allows us to connect and orchestrate many containers as one application\n",
    "- [`ec2_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config)- retrieve scrape targets from EC2 instances\n",
    "- [`kubernetes_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config) - configure scrape targets from Kubernetes REST API\n",
    "\n",
    "\n",
    "The full documentation about creating configs can be found [here](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command line\n",
    "\n",
    "> Instance of Prometheus server itself can be configured via command line flags\n",
    "\n",
    "You can run the command below to see available options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T18:54:21.063950Z",
     "start_time": "2021-04-11T18:54:20.306793Z"
    }
   },
   "outputs": [],
   "source": [
    "docker container run --rm prom/prometheus --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most notable are:\n",
    "\n",
    "- `--web.read-timeout=5m` - Maximum duration before timing out read of the request, and closing idle connections\n",
    "- `--web.max-connections=512` - Maximum number of simultaneous connections\n",
    "- `--web.enable-lifecycle` - Enable shutdown and reload via HTTP request (requests to `/-/reload` mentioned previously)\n",
    "- `--web.page-title=\"...\"` - Change header of the webpage we ran previously\n",
    "- `--storage.tsdb.retention.time` - How long should we keep the data (default: `15` days)\n",
    "- `--storage.remote.read-concurrent-limit=10` - How many targets can be read simultaneuosly\n",
    "- `--log.level=info` - Verbosity of Prometheus server, one of `[debug, info, warn, error]` can be set\n",
    "\n",
    "__Note: Those flags have their categories first, followed by more categories (optionally) and option as the last one__\n",
    "\n",
    "Above is due to Prometheus's structure and `golang` as a language of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporters\n",
    "\n",
    "> Exporters are libraries which make it easier for you to export existing metrics from third-party systems and make them available to Prometheus to track i.e an exporter so you can track GitHub metrics.\n",
    "\n",
    "There are hundreds of exporters currently all in different states of development, the full list of the most common ones [here](https://prometheus.io/docs/instrumenting/exporters/) and a complete list of all exporters on GitHub [here](https://github.com/prometheus/prometheus/wiki/Default-port-allocations):\n",
    "- **official** - best practices and verifired by Prometheus; __always pick them if possible__\n",
    "- **unofficial** - working, not verified for best practices or may have overlapping functionalities\n",
    "- **in development** - to be released as either of the two above\n",
    "\n",
    "Important things to keep in mind:\n",
    "- __Most of the exporters occupy ports `9100`-`9999`__ and any new exporter should use it if any is available (see the github list above to see which ports exporters run on)\n",
    "- There are a few exporters outside the standard range (once again, see the github list)\n",
    "\n",
    "To learn more, we will now use one of the commonly used exporters which will Hardware and Software metrics on your Operating system, __[Node exporter](https://github.com/prometheus/node_exporter)__:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Node exporter\n",
    "\n",
    "> `Node` exporter is a single static binary one can download and run straight from the workstation\n",
    "\n",
    "- Following command will download the exporter, unpack the `.tar.gz` archive (__we assume you have Linux based system!__).\n",
    "- You may run those commands anywhere you want, cell below uses temporary directory.\n",
    "- If you have Windows, you should use [this exporter](https://github.com/prometheus-community/windows_exporter).\n",
    "- If on MacOS, run `brew install node_exporter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Linux based machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T12:50:25.449113Z",
     "start_time": "2021-04-12T12:50:19.416349Z"
    }
   },
   "outputs": [],
   "source": [
    "mkdir tmp\n",
    "cd $tmpdir\n",
    "wget https://github.com/prometheus/node_exporter/releases/download/v1.1.2/node_exporter-1.1.2.linux-amd64.tar.gz\n",
    "tar xvfz node_exporter-1.1.2.linux-amd64.tar.gz\n",
    "./node_exporter-1.1.2.linux-amd64/node_exporter --help\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Add the following alias:\n",
    "- `unpack` -> `tar xvzf`\n",
    "\n",
    "> We start three node exporters to show a few more things about Prometheus, __one would be enough to monitor your OS!__\n",
    "\n",
    "And let's start the `node_exporter` (run those inside your command line after downloading, check the comments):\n",
    "- each in separate terminal __OR__\n",
    "- each in the background (using `&` after the command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T12:54:43.437243Z",
     "start_time": "2021-04-12T12:54:43.228647Z"
    }
   },
   "outputs": [],
   "source": [
    "# By default it will bind to port 9100\n",
    "# if on Mac, and you've installed the package, just remove the ./ from the commands below before running in the same way\n",
    "\n",
    "# ./node_exporter --web.listen-address 127.0.0.1:9100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Prometheus to scrape Node exporter\n",
    "\n",
    "Now that we have our exporters running we need to setup `Prometheus` server to scrape data from them.\n",
    "\n",
    "Let's change our `prometheus.yml` config file to contain the following setup, which will allow the `Node exporter` to scrape metrics from our system.\n",
    "\n",
    "### For linux/MAC systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global:\n",
    "  scrape_interval: '1s'  # By default, scrape targets every 15 seconds.\n",
    "  external_labels:\n",
    "    monitor: 'codelab-monitor'\n",
    "\n",
    "scrape_configs:\n",
    "  # Prometheus monitoring itself\n",
    "  - job_name: 'prometheus'\n",
    "    scrape_interval: '10s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "  # OS monitoring\n",
    "  - job_name: 'node'\n",
    "    scrape_interval: '5s'\n",
    "    static_configs:\n",
    "      - targets: ['prometheus:9100']\n",
    "        labels:\n",
    "          group: 'production' # notice we have defined two nodes to be labelled in the production environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Windows systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global:\n",
    "  scrape_interval: '5s'  # By default, scrape targets every 15 seconds.\n",
    "  external_labels:\n",
    "    monitor: 'codelab-monitor'\n",
    "\n",
    "scrape_configs:\n",
    "  # Prometheus monitoring itself\n",
    "  - job_name: 'prometheus'\n",
    "    scrape_interval: '10s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "  # OS monitoring\n",
    "  - job_name: 'wmiexporter'\n",
    "    scrape_interval: '30s'\n",
    "    static_configs:\n",
    "      - targets: ['YOUR PUBLIC IP HERE:9182']\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the basic layout when defining new targets to scrape is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_configs: # all targets will be defined in a config\n",
    "  - job_name: 'prometheus' # name of the job\n",
    "    scrape_interval: '10s' #parameters defining how to scrape the target.\n",
    "    static_configs:              # allow you to specify a list of targets    \n",
    "      - targets: ['localhost:9090']  # define the targets here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to give this config file to `Prometheus` server (__it is contained in Docker, remember!__), there are two options for this we have already implemented the first one which allows us to edit the config file while the server is running. \n",
    "\n",
    "There are two ways to do that:\n",
    "- Mount directory in your `localhost` to Docker container during runtime: \n",
    "    - When configuration is changing often (and you have autoreload set)\n",
    "- Create new Docker image and copy the configuration:\n",
    "    - When configuration is static and changing rarely\n",
    "    \n",
    "We will using the first approach editing the configuration file locally and then uploading it to the server while it is running. You can find the documentation to the second approach using a `Dockerfile` on the following [page](https://prometheus.io/docs/prometheus/latest/installation/). \n",
    "\n",
    "Now that our `prometheus.yml` has been updated locally we can push it to the Prometheus server using the same command as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:9090/-/reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Go to [`localhost:9090/targets`](http://localhost:9090/targets) to verify everything was set up correctly:__ you can see on the targets dashboard that both of our targets have the state **UP** so will be collecting metrics, targets can also in other predefined state.\n",
    "\n",
    "- **Down** Prometheus cannot connect to the target.\n",
    "- **Unknown** Prometheus doesn't know where to find the target, usually due to a configuration issue.\n",
    "\n",
    "<img src=\"images/prom_target_dash.png?modified=232132453\">\n",
    "\n",
    "From the targets window we can click on the endpoint of the target in our cases for windows exporter. `http://192.168.8.50:9182/metrics` to get a text list of all avalible metrics which are being scraped and avaliable to Prometheus. This can be helpful if you are unsure what metrics are avaliable to you. \n",
    "\n",
    "<img src=\"images/prom_metrics_endpoint.png?modified=232132453\">\n",
    "\n",
    "Once done, go to [`localhost:9090`](http://localhost:9090) and check whether you have the commands prefixed with `node` or `windows` available:__\n",
    "\n",
    "<img src=\"images/prom_win_commands.png?modified=23212453\">\n",
    "\n",
    "Run a few expressions to see the result of the graphs a good one for windows will be `windows_os_processes` notice we can see the amount of processes running every 5 seconds by hovering over the graph.   \n",
    "\n",
    "<img src=\"images/prom_win_process.png?modified=23212453\">\n",
    "\n",
    "__In the next lesson, we will learn how to query our data efficiently__, but before that.. let's show you how to monitor metrics from Docker containers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Prometheus to track Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly to begin getting Prometheus to track Docker we will need to edit Dockers configuration so that we can specify it's metric address so Prometheus knows where to collect the metrics. We need to do this by editing the `daemon.json` docker file or by editing Docker Desktops configuration file.\n",
    "\n",
    "- **For Linux:** Navigate to `/etc/docker/daemon.json`\n",
    "- **For MAC/Windows:** Go to Docker Desktop, click the cog to go to **settings > Docker Engine**.\n",
    "\n",
    "Add this code to configure scraping of metrics either the `daemon.json` file or to the Docker Engine config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"metrics-addr\" : \"0.0.0.0:9323\",\n",
    "  \"experimental\" : true\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Docker Desktop it will likely ask you to restart your Docker Desktop application. On Linux you will need to restart Docker `service docker restart` on Linux. Great now we just need to update our `Prometheus.yml` to add docker as a target so Prometheus can begin scraping the metrics. Add docker as a target in your `Prometheus.yml` like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global:\n",
    "  scrape_interval: '15s'  # By default, scrape targets every 15 seconds.\n",
    "  scrape_timeout: '10s'\n",
    "  external_labels:\n",
    "    monitor: 'codelab-monitor'\n",
    "\n",
    "scrape_configs:\n",
    "  # Prometheus monitoring itself\n",
    "  - job_name: 'prometheus'\n",
    "    scrape_interval: '10s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "  # OS monitoring\n",
    "  - job_name: 'wmiexporter'\n",
    "    scrape_interval: '30s'\n",
    "    static_configs:\n",
    "      - targets: ['YOU LOCAL IP HERE:9182']\n",
    "\n",
    "  - job_name: 'docker'\n",
    "         # metrics_path defaults to '/metrics'\n",
    "         # scheme defaults to 'http'.\n",
    "\n",
    "    static_configs:\n",
    "      - targets: ['YOUR LOCAL IP HERE:9323']\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now like before update the config on the Promtheus server while it's running using the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:9090/-/reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the Promethus targets pane from our server and see that docker is now avaliable as an endpoint to collect metrics. Remember to check its endpoint/metrics to see a list of avaliable metrics to track. From docker they will usually start with `engine_daemon_container`.\n",
    "\n",
    "<img src=\"images/prom_targets_docker.png?modified=232132453\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try starting some containers either start from Prometheus docker containers like before or use Docker hub to create some containers. Once they are running you can run the expressions to view the output of the metrics. For example here is the result after starting and stopping some containers from the `engine_daemon_container_states_containers` expression. Where the red line indicates containers stopping and the blue indicates containers are being run.\n",
    "\n",
    "<img src=\"images/prom_cont_states.png?modified=232132453\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- We've learned to start a Prometheus Server inside a Docker container.\n",
    "- Learned how to access the Prometheus dashboard and run some query expressions.\n",
    "- How to create a `Prometheus.yml` configuration for our server which we can update while the server is running live.\n",
    "- We have integrated the node/windows exporter with the server so we can track hardware/software metrics from a machine.\n",
    "- We have also setup prometheus to track the docker service and learned how to define targets to track in the `prometheus.yml` config file. \n",
    "\n",
    "### Additional \n",
    "\n",
    "- What are the available alternatives to Prometheus? Read about them [here](https://prometheus.io/docs/introduction/comparison/#). When should we use a different tool? \n",
    "- What is Prometheus's [Push Gateway](https://prometheus.io/docs/practices/pushing/)? When should we use it?\n",
    "- Read a little bit about alerting in Prometheus (e.g. when disaster happens it can send you an e-mail). Go through [documentation](https://prometheus.io/docs/alerting/latest/overview/) and get a basic grasp (leave alerting rules until the next lesson though)\n",
    "- Local disk storage is limited. How can one integrate with Prometheus's remote storage? Read about it [here](https://prometheus.io/docs/prometheus/latest/storage/).\n",
    "- What is Prometheus Federation? Read about it [here](https://prometheus.io/docs/prometheus/latest/federation/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash [conda env:.conda-AiCore] *",
   "language": "bash",
   "name": "conda-env-.conda-AiCore-bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
